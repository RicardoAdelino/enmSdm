% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trainByCrossValid.r
\name{trainByCrossValid}
\alias{trainByCrossValid}
\title{Calibrate a distribution/niche model using cross-validation}
\usage{
trainByCrossValid(data, resp = names(data)[1],
  preds = names(data)[2:ncol(data)], folds = dismo::kfold(data),
  trainFx = enmSdm::trainGlm, ..., metrics = c("cbi", "auc", "fpb",
  "tss", "msss", "mdss", "minTrainPres", "trainPres05", "trainPres10"),
  w = NULL, weightEvalTrain = TRUE, weightEvalTest = TRUE,
  out = c("models", "tuning"), verbose = 1)
}
\arguments{
\item{data}{Data frame or matrix. Environmental predictors (and no other fields) for presences and background sites.}

\item{resp}{Character or integer. Name or column index of response variable. Default is to use the first column in \code{data}.}

\item{preds}{Character list or integer list. Names of columns or column indices of predictors. Default is to use the second and subsequent columns in \code{data}.}

\item{trainFx}{Function, name of the "trainXYZ" function to use (e.g., \code{\link[enmSdm]{trainGlm}} or \code{\link[enmSdm]{trainGam}}).}

\item{...}{Arguments to pass to the "trainXYZ" function and \code{\link{predictEnmSdm}} functions. See help for the appropriate function.}

\item{weightEvalTrain}{Logical, if \code{TRUE} (default) and an argument named \code{w} is specified in \code{...}, then evaluation statistics that support weighting will use the weights specified by \code{w} \emph{for the "train" version of evaluation statistics}. If \code{FALSE}, there will be no weighting of test sites. Note that this applies \emph{only} to the calculation of evaluation statistics.  If \code{w} is supplied weights they will be used for model calibration.}

\item{weightEvalTest}{Logical, if \code{TRUE} and an argument named \code{w} is specified in \code{...}, then evaluation statistics that support weighting will use the weights specified by \code{w} \emph{for the "test" version of evaluation statistics}. If \code{FALSE} (default), there will be no weighting of test sites. Note that this applies \emph{only} to the calculation of evaluation statistics.  If \code{w} is supplied then weights will be used for model calibration.}

\item{out}{Character. Indicates type of value returned. If \code{model} then returns a model object (e.g., of class \code{MaxEnt}, \code{glm}, etc.). If \code{tuning} then just return the evaluation table for each kind of model term used in model construction. If both then return a 2-item list with the best model and the tuning table.}

\item{verbose}{Numeric. If 0 show no progress updates. If > 0 then show minimal progress updates for this function only. If > 1 show detailed progress for this function. If > 2 show detailed progress plus detailed progress for the "trainXYZ" function.}

\item{na.rm}{Logical, if \code{TRUE} then remove \code{NA} predictions before calculating evaluation statistics. If \code{FALSE} (default), propagate \code{NA}s (meaning if predictions contain \code{NA}s, then the evaluation statistic will most likely also be \code{NA}.)}
}
\value{
If \code{out = 'models'} this function returns a list with models. If If \code{out = 'tuning'} this function returns a data frame with performance statistics against the training and testing data set. If \code{out = c('model', 'tuning'} then it returns a list object with the models and tuuning statistics.
}
\description{
This function is an extension of any of the "trainXYZ" functions for calibrating species distribution and ecological niche models. The "trainXYZ" functions use various criteria to select a "best" model across a suite of models using different sets of variables and/or functional forms and parameterizations. This function uses the "trainXYZ" function to train a suite of models typically calibrated on somewhat non-overlapping data.  The models are evaluated against withheld data to determine the optimal settings for a "final" model using all available data.
}
\examples{
set.seed(123)

### contrived example
n <- 10000
x1 <- seq(-1, 1, length.out=n) + rnorm(n)
x2 <- seq(10, 0, length.out=n) + rnorm(n)
x3 <- rnorm(n)
y <- 2 * x1 + x1^2 - 10 * x2 - x1 * x2

y <- statisfactory::probitAdj(y, 0)
y <- y^3
hist(y)

presAbs <- runif(n) < y

trainData <- data.frame(presAbs=presAbs, x1=x1, x2=x2, x3=x3)

model <- trainGlm(trainData)

out <- trainByCrossValid(data=trainData, verbose=2)
out$tuning
lapply(out$models, coefficients)
lapply(out$models, logLik)
}
\seealso{
\code{\link[enmSdm]{trainBrt}}, \code{\link[enmSdm]{trainCrf}}, \code{\link[enmSdm]{trainGam}}, \code{\link[enmSdm]{trainGlm}}, \code{\link[enmSdm]{trainMaxEnt}}, \code{\link[enmSdm]{trainLars}}, \code{\link[enmSdm]{trainMaxNet}}, \code{\link[enmSdm]{trainRf}}
}
